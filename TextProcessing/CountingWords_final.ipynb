{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment: Count Most Frequently used words in Veidenbaums.txt\n",
    "# For English let's use Alice in Wonderland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open File - Get Data\n",
    "# Read Text\n",
    "# Split Text into word tokens\n",
    "# Count these tokens (we need to figure out how to)\n",
    "# Save/Print Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File is under /data/Veidenbaums.txt\n",
    "# Alice is under /data/Alice_Wonderland.txt\n",
    "# we are under /TextProcessing/CountingWords.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This means one level up and then again down into data\n",
    "# \"../data/Veidenbaums.txt\"\n",
    "# So called relative path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "164410"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filePath = \"../data/Veidenbaums.txt\"\n",
    "filePath = \"../data/Alice_Wonderland.txt\"\n",
    "with open(filePath, encoding=\"utf-8\") as fstream: # openign a filestream\n",
    "    mytext = fstream.read() # here our textual data is read into memory\n",
    "# here fstream is closed automatically for you\n",
    "len(mytext) # how many symbols are in our text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ufeffThe Project Gutenberg EBook of Alice’s Adventures in Wonderland, by Lewis Carroll\\n\\nThis eBook is for the use of anyone '"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mytext[:120] # first 120 symbols from our text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'-tm,\\nincluding how to make donations to the Project Gutenberg Literary\\nArchive Foundation, how to help produce our new eBooks, and how to\\nsubscribe to our email newsletter to hear about new eBooks.\\n\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mytext[-200:] # last two hundred symbols of our text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so one issue is that pretty much any data source will need additional cleaning\n",
    "# here we have header and footer sections with some legal data \n",
    "# this meta-data would distort our analysis\n",
    "# for one we would get mentions of Mr. Gutenberg \n",
    "# which Mr. Lewis Carrol did not intend\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145459"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so we need to do some cleaning first\n",
    "# ideas on getting rid of header and footer sections\n",
    "#\n",
    "mytext.index(\"End of Project Gutenberg\") \n",
    "#this will the index of the first mention of the string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145459"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanend = mytext[:mytext.index(\"End of Project Gutenberg\")] \n",
    "# so this will cut and save text to last character\n",
    "# before the above text is found\n",
    "len(cleanend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'of Wonderland of long ago: and how she\\nwould feel with all their simple sorrows, and find a pleasure in all\\ntheir simple joys, remembering her own child-life, and the happy summer\\ndays.\\n\\nTHE END \\n\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanend[-200:] # last 200 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "729"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_index = cleanend.index(\"START OF THIS PROJECT GUTENBERG EBOOK\")\n",
    "start_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skip = len(\"START OF THIS PROJECT GUTENBERG EBOOK\")\n",
    "skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' ALICE’S ADVENTURES IN WONDERLAND ***\\n\\n\\n\\nProduced by Arthur DiBianca and David Widger\\n\\n[Illustration]\\n\\n\\n\\n\\nAlice’s Adventures in Wonderland\\n\\nby Lewis C'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanall = cleanend[start_index+skip:] # so we want everything from the index+skip\n",
    "cleanall[:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Alice_clean.txt', mode=\"w\", encoding=\"utf-8\") as file_out:\n",
    "    file_out.write(cleanall)\n",
    "# so this recipe will create/overwrite 'Alice_clean.txt'\n",
    "# using utf-8 encoding\n",
    "# will write ALL of the text that is in cleanall variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/Alice_clean.txt', mode=\"w\", encoding=\"utf-8\") as file_out:\n",
    "    file_out.write(cleanall)\n",
    "# so this recipe will create/overwrite 'Alice_clean.txt' in data\n",
    "# using utf-8 encoding\n",
    "# will write ALL of the text that is in cleanall variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' ALICE’S ADVENTURES IN WONDERLAND ***\\n\\n\\n\\nProduced by Arthur DiBianca and David Widger\\n\\n[Illustration'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets pretende i do not have cleanall in memory\n",
    "# i can load text from my clean file\n",
    "with open('../data/Alice_clean.txt', mode=\"r\", encoding=\"utf-8\") as file:\n",
    "    mytext = file.read() # so reads everything\n",
    "mytext[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so let's see about tokenization \n",
    "# one is to use split using white space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A quick brown fox    jumped \t over the   \n",
      " \n",
      " sleepy dog\n"
     ]
    }
   ],
   "source": [
    "mysentence = \"A quick brown fox    jumped \\t over the   \\n \\n sleepy dog\"\n",
    "print(mysentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'sleepy', 'dog']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mysentence.split()  # so we split by white space, including newlines and tabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24630"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we could try splitting already but we will get dirty data(words)\n",
    "mywords = mytext.split(\" \")\n",
    "len(mywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'ALICE’S',\n",
       " 'ADVENTURES',\n",
       " 'IN',\n",
       " 'WONDERLAND',\n",
       " '***\\n\\n\\n\\nProduced',\n",
       " 'by',\n",
       " 'Arthur',\n",
       " 'DiBianca',\n",
       " 'and',\n",
       " 'David',\n",
       " 'Widger\\n\\n[Illustration]\\n\\n\\n\\n\\nAlice’s',\n",
       " 'Adventures',\n",
       " 'in',\n",
       " 'Wonderland\\n\\nby',\n",
       " 'Lewis',\n",
       " 'Carroll\\n\\nTHE',\n",
       " 'MILLENNIUM',\n",
       " 'FULCRUM',\n",
       " 'EDITION',\n",
       " '3.0\\n\\nContents\\n\\n',\n",
       " 'CHAPTER',\n",
       " 'I.',\n",
       " '',\n",
       " '']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mywords[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so we need to get rid of some \\n which split did not\n",
    "# so how could we do this?\n",
    "# replace to rescue!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' ALICE’S ADVENTURES IN WONDERLAND ***    Produced by Arthur DiBianca and David Widger  [Illustration'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text = mytext.replace(\"\\n\", \" \") # why not nothing \"\"?\n",
    "# replacing with \"\" we run the risk of combining words\n",
    "clean_text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ALICE’S',\n",
       " 'ADVENTURES',\n",
       " 'IN',\n",
       " 'WONDERLAND',\n",
       " '***',\n",
       " 'Produced',\n",
       " 'by',\n",
       " 'Arthur',\n",
       " 'DiBianca',\n",
       " 'and',\n",
       " 'David',\n",
       " 'Widger',\n",
       " '[Illustration]',\n",
       " 'Alice’s',\n",
       " 'Adventures',\n",
       " 'in',\n",
       " 'Wonderland',\n",
       " 'by',\n",
       " 'Lewis',\n",
       " 'Carroll']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = clean_text.split()\n",
    "words[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we could do additionalal cleaning by checking for maybe bad words\n",
    "# bad characters\n",
    "# unneeded words such stop words , meaning words which do not contribute to the meaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so how could we count the occurences of each word?\n",
    "# we could use something like a dictionary with words being keys and value being \n",
    "# number of occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ALICE’S', 1),\n",
       " ('ADVENTURES', 1),\n",
       " ('IN', 1),\n",
       " ('WONDERLAND', 1),\n",
       " ('***', 1),\n",
       " ('Produced', 1),\n",
       " ('by', 55),\n",
       " ('Arthur', 1),\n",
       " ('DiBianca', 1),\n",
       " ('and', 718),\n",
       " ('David', 1),\n",
       " ('Widger', 1),\n",
       " ('[Illustration]', 1),\n",
       " ('Alice’s', 11),\n",
       " ('Adventures', 2),\n",
       " ('in', 346),\n",
       " ('Wonderland', 2),\n",
       " ('Lewis', 1),\n",
       " ('Carroll', 1),\n",
       " ('THE', 3)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dict = {}\n",
    "for word in words:\n",
    "    if word in word_dict.keys():\n",
    "        word_dict[word] += 1 # increase count for each occurence\n",
    "    else:\n",
    "        word_dict[word] = 1 # so first occurence you count as 1\n",
    "    # below is just a shorter version of the above\n",
    "#     word_dict[word] = word_dict.get(word, 0) + 1\n",
    "list(word_dict.items())[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 1515),\n",
       " ('and', 718),\n",
       " ('to', 706),\n",
       " ('a', 611),\n",
       " ('of', 493),\n",
       " ('she', 485),\n",
       " ('said', 416),\n",
       " ('it', 347),\n",
       " ('in', 346),\n",
       " ('was', 327)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter # python already provides counter!\n",
    "word_count = Counter(words) # give a list of tokens\n",
    "word_count.most_common(10) # and you can get results immediately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so now that we have some prelimenary results in\n",
    "# we could start thinking about stripping stopwords\n",
    "# maybe getting rid of very short words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "485"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count[\"she\"] # turns out Counter is just a dictionary with some benefits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count[\"he\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"word_count.tsv\", mode=\"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"word\\tcount\\n\") # so we add newlines by hand\n",
    "    for word,count in word_count.most_common():\n",
    "        f.write(f\"{word}\\t{count}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can use libraries such as CSV and Pandas to take care of writing \n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to continue we will need to perform some additional cleaning\n",
    "# maybe think about some visualization of some of this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will need to clean all lines which contain *** as ending characters\n",
    "# so lets try reading lines\n",
    "# filePath = \"../data/Veidenbaums.txt\"\n",
    "# with open(filePath, encoding=\"utf-8\") as fstream:\n",
    "#     mylines = fstream.readlines()\n",
    "# len(mylines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mylines[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanlines = [line for line in mylines if line[0]!='\\n']\n",
    "len(cleanlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanlines[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we do not want the lines which end with ***\\n\n",
    "headlines = [line for line in cleanlines if line.endswith(\"***\\n\")]\n",
    "headlines[:5]\n",
    "# we do not need the headlines!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we do not want the lines which end with ***\\n\n",
    "noheadlines = [line for line in cleanlines if not line.endswith(\"***\\n\")]\n",
    "noheadlines[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we could save the results \n",
    "savePath = \"../data/noHeadVeidenbaums.txt\"\n",
    "with open(savePath, mode=\"w\", encoding=\"utf-8\") as fstream:\n",
    "    fstream.writelines(noheadlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "366"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# May 6th lets start with noheadlines\n",
    "myPath = \"../data/noHeadVeidenbaums.txt\"\n",
    "with open(myPath, encoding=\"utf-8\") as fstream:\n",
    "    noheadlines = fstream.readlines()\n",
    "len(noheadlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "365"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "noheadlines = [line for line in noheadlines if not \"Treimanim\" in line]\n",
    "len(noheadlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "spaceChars = \"\\n-\"\n",
    "stopChars = \"\"\"!?.,\"':;()…\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for char in stopChars:\n",
    "    print(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pēc ideāliem cenšas lielie gari,\\nBet dzīvē ieņemt vietu'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One big text from many lines\n",
    "textNoHead = \"\".join(noheadlines) # we could have used fstream.read earlier\n",
    "textNoHead[:55]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replacing \n",
      " with space\n",
      "Replacing - with space\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Pēc ideāliem cenšas lielie gari, Bet dzīvē ieņemt vietu pirmie Tie neiespēj'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take off spacy Characters replace with space (why space ? :)\n",
    "for char in spaceChars:\n",
    "    print(f\"Replacing {char} with space\")\n",
    "    textNoHead = textNoHead.replace(char, \" \")\n",
    "#     print(textNoHead[:75])\n",
    "textNoHead[:75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replacing ! with nothing\n",
      "Replacing ? with nothing\n",
      "Replacing . with nothing\n",
      "Replacing , with nothing\n",
      "Replacing \" with nothing\n",
      "Replacing ' with nothing\n",
      "Replacing : with nothing\n",
      "Replacing ; with nothing\n",
      "Replacing ( with nothing\n",
      "Replacing ) with nothing\n",
      "Replacing … with nothing\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Pēc ideāliem cenšas lielie gari Bet dzīvē ieņemt vietu '"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for char in stopChars:\n",
    "    print(f\"Replacing {char} with nothing\")\n",
    "    textNoHead = textNoHead.replace(char, \"\")\n",
    "textNoHead[:55]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "savePath = \"../data/noHeadVeidenbaumsOneLine.txt\"\n",
    "with open(savePath, mode=\"w\", encoding=\"utf-8\") as fstream:\n",
    "    fstream.write(textNoHead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "substring not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-cadf8a59266b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtextNoHead\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Vēstule\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# nothing found thats good\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: substring not found"
     ]
    }
   ],
   "source": [
    "textNoHead.index(\"Vēstule\")\n",
    "# nothing found thats good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eiz zaļoja jaunība cerības pla'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textNoHead[5400:5430]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'G',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'Z',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'z',\n",
       " 'Ā',\n",
       " 'ā',\n",
       " 'č',\n",
       " 'Ē',\n",
       " 'ē',\n",
       " 'ģ',\n",
       " 'Ī',\n",
       " 'ī',\n",
       " 'ķ',\n",
       " 'ļ',\n",
       " 'ņ',\n",
       " 'Š',\n",
       " 'š',\n",
       " 'ū',\n",
       " 'ž'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# charSet that's Camelcase another style\n",
    "char_set = set(textNoHead)\n",
    "char_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8230"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord(\"…\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pēc', 'ideāliem', 'cenšas', 'lielie', 'gari']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = textNoHead.split()\n",
    "words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pēc', 'ideāliem', 'cenšas', 'lielie', 'gari']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we need to convert to lower case\n",
    "# for word in words:\n",
    "words_lower = [word.lower() for word in words]\n",
    "words_lower[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1865"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we want to do it ourselves\n",
    "# we could store it in a dictionary word and count\n",
    "# {'pēc':5, 'ideālie':1, 'cenšas':3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1064"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_words = set(words_lower)\n",
    "len(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# i create a dictionary of unique words and set counter to 0\n",
    "my_counter_dict = {word:0 for word in list(unique_words)}\n",
    "my_counter_dict['pēc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in words_lower:\n",
    "    my_counter_dict[word] += 1 # each time i add 1 to right box(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_counter_dict['pēc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('apkakli', 1), ('šūpulis', 1), ('aša', 1), ('līdz', 8), ('tiesas', 1)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_list_tuples = [(key, value) for key,value in my_counter_dict.items()]\n",
    "my_list_tuples[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('acs', 1), ('agrāk', 1), ('aiz', 1), ('aizgāja', 1), ('aizmirsts', 1)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(my_list_tuples)[:5]\n",
    "# not quite what we need because it sorts by the first item alphabetically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('un', 76),\n",
       " ('ir', 24),\n",
       " ('vēl', 22),\n",
       " ('tu', 21),\n",
       " ('tik', 21),\n",
       " ('bet', 15),\n",
       " ('kas', 15),\n",
       " ('nav', 14),\n",
       " ('man', 14),\n",
       " ('kā', 13)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# solution we pass a function to show how to sort\n",
    "my_most_common = sorted(my_list_tuples,key=lambda mytuple: mytuple[1], reverse=True)\n",
    "my_most_common[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so sorting is possible but my recommendation is to use Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# well and now I would to like sort\n",
    "# its possible then I need to create a list from dictionary and then sort by key value\n",
    "# solution use a library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batteries are included no need to write our own counter\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "mycounter = Counter(words_lower)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('un', 76),\n",
       " ('ir', 24),\n",
       " ('vēl', 22),\n",
       " ('tik', 21),\n",
       " ('tu', 21),\n",
       " ('bet', 15),\n",
       " ('kas', 15),\n",
       " ('nav', 14),\n",
       " ('man', 14),\n",
       " ('par', 13)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mycounter.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(mycounter.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1283"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how to get only words 4 chars or longer ? :)\n",
    "long_words = [word for word in words_lower if len(word) >= 4 ]\n",
    "len(long_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('reiz', 10),\n",
       " ('viss', 9),\n",
       " ('līdz', 8),\n",
       " ('mums', 7),\n",
       " ('sauc', 6),\n",
       " ('gars', 6),\n",
       " ('projām', 6),\n",
       " ('laiks', 5),\n",
       " ('sirds', 5),\n",
       " ('tomēr', 5)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_counter = Counter(long_words)\n",
    "long_counter.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'alus' in long_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "collections.Counter"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(long_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_counter.get('alus'), long_counter['alus'] #2nd would throw error if no beer existed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('laiks', 5),\n",
       " ('sirds', 5),\n",
       " ('tomēr', 5),\n",
       " ('likumīgi', 5),\n",
       " ('dzīves', 5),\n",
       " ('iedzer', 5)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we only get 5 letter words here\n",
    "word_counter_5 = [mytuple for mytuple in long_counter.most_common() if mytuple[1] == 5]\n",
    "word_counter_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('most_common.json', mode='w', encoding='utf-8') as fstream:\n",
    "    json.dump(mycounter.most_common(), fstream, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we want to save our Latvian or other languages besides ENglish we set\n",
    "# turn off ascii\n",
    "# https://stackoverflow.com/questions/18337407/saving-utf-8-texts-in-json-dumps-as-utf8-not-as-u-escape-sequence\n",
    "with open('most_common.json', mode='w', encoding='utf-8') as fstream:\n",
    "    json.dump(mycounter.most_common(), fstream, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
